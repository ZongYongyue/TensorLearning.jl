{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Tensor Contractions \n",
    "source: https://www.tensors.net/tutorial-1\n",
    "\n",
    "- Initialization of tensors\n",
    "\n",
    "- Diagrammatic notation for tensors and tensor networks\n",
    "\n",
    "- Manipulation of tensors via 'permute' and 'reshape' functions\n",
    "\n",
    "- Binary tensor contractions and computational costs\n",
    "\n",
    "- Use of 'ncon' routine to contract networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.1: Diagrammatic notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our present purpose, tensors are simply multi-dimensional arrays of (real or complex) numbers. Below are some useful ways to initialize tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lets initialize some tensors in Julia\n",
    "using LinearAlgebra\n",
    "# tensor with randomly generated entries, order 3, dims: 2-by-3-by-4\n",
    "A = rand(2,3,4)\n",
    "\n",
    "# identity matrix, order 2, dims: 5-by-5 (New syntax in Julia 0.7+)\n",
    "B = Matrix{Float64}(I,5,5)\n",
    "\n",
    "# tensor of 1's, order 4, dims: 2-by-4-by-2-by-4\n",
    "C = ones(2,4,2,4)\n",
    "\n",
    "# matrix of 0's, order 2, dims: 3-by-5\n",
    "D = zeros(3,5)\n",
    "\n",
    "# initialize complex random tensor\n",
    "E = rand(2,3,4) + im*rand(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to represent tensor networks using a **diagrammatic notation**, where individual tensors are represented as a solid shape with a number of 'legs' that corresponds to the rank of the tensor. Each leg is here labelled with a dummy index (usually a Latin letter: i, j, k, l…) necessary to relate the equation to the diagram. Some examples are presented below.\n",
    "\n",
    "<img  src=\"./Figs/Fig.1.1(a).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagrammatic tensor notation is useful for describing networks comprised of multiple tensors. An index shared by two tensors denotes a contraction (or summation) over this index. Examples:\n",
    "\n",
    "<img  src=\"./Figs/Fig.1.1(b,c).png\"  width=\"600\"  align=\"center\" />\n",
    "\n",
    "\n",
    "Notice that example in Fig.1.1(b) is equivalent to a matrix multiplication between matrices **A** and **B**, while Fig.1.1(c) produces a rank-3 tensor **D** via the contraction of a network with three tensors. Even in this relatively the simple example, we see that the diagrammatic notation is already easier to interpret than the corresponding index equation. In practice, once we have established a convention for index ordering, we can omit labeling the diagrams with dummy indices which further enhances their clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.2: <font color=red>Permute</font> and <font color=red>reshape</font> operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Fig.1.2.png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.2(a):Permute\n",
    "A = rand(4,4,4,4)\n",
    "Atilda = permutedims(A,[4,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.2(b):Reshape\n",
    "B = rand(4,4,4)\n",
    "Btilda = reshape(B,4,4^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical notes:\n",
    "\n",
    "- The tensor reshape behaves differently in MATLAB/Julia versus Python due to a difference in convention. Both MATLAB and Julia use column-major order for storing matrices and tensors, such that a d-by-d matrix Bij is stored as a length d^2 vector vk, with k = i + (j-1)×d. In contrast, Python uses row-major order such that a d-by-d matrix Bij is stored as a vector vk, with k = i×d + j. Fortunately this difference in convention does not often have significant consequences in terms of writing tensor network codes, since the choice of convention is not so important so long as it is consistently applied.\n",
    "\n",
    "- The permute function reorders the storage of the elements of a tensor in computer memory, thus incurs some (often non-negligible) computational cost. In contrast, the reshape function leaves the elements of a tensor unchanged in memory, instead only changing the metadata for how the tensor is to be interpreted (and thus incurs negligible cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.3: Binary tensor contractions\n",
    "\n",
    "The usefulness of permute and reshape functions is that they allow a contraction between a pair of tensors (which we call a binary tensor contraction) to be recast as a matrix multiplication. Although the computational cost (measured in number of scalar multiplications) is the same both ways, it is usually preferable to recast as multiplication as modern hardware performs vectorized operations much faster than when using the equivalent FOR loop. The steps for doing this are outlined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Fig.1.3.png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.3(a): Binary Tensor Contraction\n",
    "d = 10;\n",
    "A = rand(d,d,d,d);  B = rand(d,d,d,d);\n",
    "Ap  = permutedims(A,[1,3,2,4]);  Bp  = permutedims(B,[1,4,2,3]);\n",
    "App = reshape(Ap,d^2,d^2);       Bpp = reshape(Bp,d^2,d^2);\n",
    "Cpp = App*Bpp;\n",
    "C   = reshape(Cpp,d,d,d,d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.4: Contraction costs\n",
    "\n",
    "The computational cost of multiplying a d1-by-d2 dimensional matrix **A** with a d2-by-d3 dimensional matrix **B** is: cost(A×B) = d1∙d2∙d3. Given the equivalence with matrix multiplication, this is also the cost of a binary tensor contraction (where each dimension d1, d2, d3 may now result as the product of several tensor indices from the reshapes).\n",
    "\n",
    "Another way of computing the cost of contracting **A** and **B** is to take the product of the total dimensions, denoted |dim(**A**)| and |dim(**B**)|, of each tensor divided by the total dimension of the contracted indices, denoted |dim(**A**∩**B**)|. Examples are given below:\n",
    "\n",
    "<img  src=\"./Figs/Fig.1.4.png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, there are two approaches that could be taken to contract a network containing N>2 tensors: (i) in a single step as a direct summation over all internal indices of the network or (ii) as a sequence of N-1 binary contractions. In practice we prefer the latter option, which is either computationally cheaper or an equivalent cost as the former option. Examples:\n",
    "\n",
    "<img  src=\"./Figs/Fig.1.4(c).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig.1.4(c), which represents a product of three matrices, illustrates that it is more efficient (in terms of the total number of scalar multiplications) to evaluate the network as a sequence of binary contractions than as a single summation over all internal indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.4(c): Tensor network evaluation\n",
    "d = 10; A = rand(d,d);  B = rand(d,d); C = rand(d,d);\n",
    "# Evaluare network via summation over internal indices\n",
    "F0 = zeros(d,d);\n",
    "for id = 1:d\n",
    "    for jd = 1:d\n",
    "        for kd = 1:d\n",
    "            for ld = 1:d\n",
    "                F0[id,jd] = F0[id,jd] + A[id,kd]*B[kd,ld]*C[ld,jd];\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "# Evaluare network via sequence of binary contractions\n",
    "F1 = (A*B)*C;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Fig.1.4(d).png\"  width=\"600\"  align=\"center\" />\n",
    "\n",
    "\n",
    "Fig.1.4(d) illustrates that the total cost of contracting a tensor network can depend on the sequence of binary contractions used; here the optimal sequence depends on whether D is larger than d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.5: Contraction of tensor networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given a tensor network composed of N tensors, there are two distinct steps needed to contract the network efficiently:\n",
    "\n",
    "- determine the optimal sequence of the (N-1) binary tensor contractions,\n",
    "\n",
    "- evaluate each of the binary contractions in turn as a matrix multiplication by taking the proper tensor permutes and reshapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notes: determining the optimal contraction sequence:\n",
    "\n",
    "Usually we refer to the ‘optimal’ sequence at that which minimizes the number of scalar multiplications, but one could also seek to minimize the size of intermediate tensors used in the contraction (if the calculation was memory limited). Often, though not always, these two criteria will coincide.\n",
    "\n",
    "Given a tensor network with only a few tensors it is often easy to find the optimal sequence ‘manually’ through inspection of the network. For more complicated networks with a large number of tensors it may be necessary to employ an automated search algorithm such as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Fig.1.5(a).png\"  width=\"600\"  align=\"center\" />\n",
    "\n",
    "Once the optimal contraction sequence has been determined, a network can be evaluated by implementing each of the binary contractions in turn. However, using ‘reshape’ and ‘permute’ commands for each binary tensor contraction, although computationally effective, has two significant drawbacks: \n",
    "- (i) it results in lengthy code that is error prone and difficult to check\n",
    "- (ii) it does not allow for the contraction sequence to be easily changed (as, in general, the entire code for the contraction would need to be rewritten to accommodate a different ordering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network contractor ‘ncon’:\n",
    "\n",
    "The ‘ncon’ function is a useful tool to lessen the programming effort required to implement a tensor network contraction. This function works by automatically performing a desired sequence of permutes, reshapes and matrix multiplications required to evaluate a tensor network. The ‘ncon’ code and detailed instructions for its usage can be found here, or alternatively the code is also presented on the example code page. The first step in using ‘ncon’ to evaluate a network is to make a labelled diagram of the network such that:​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each internal index is labelled with a unique positive integer (typically sequential integers starting from 1, although this is not necessary).\n",
    "\n",
    "- External indices of the diagram (if there are any) are labelled with sequential negative integers [-1,-2,-3,…] which denote the desired index order on the final tensor (with -1 as the first index, -2 as the second etc).\n",
    "\n",
    "​Following this, the ‘ncon’ routine is called as follows,\n",
    "\n",
    " <font color=red> **OutputTensor = ncon(TensorArray, IndexArray, ContOrder),** </font>\n",
    "\n",
    " with input arguments defined:\n",
    "\n",
    "- **TensorArray**: 1D cell array containing the tensors comprising the network\n",
    "\n",
    "- **IndexArray**: 1D cell array of vectors, where the kth element is a vector of the integer labels from the diagram on the kth tensor from ‘TensorArray’ (ordered following the corresponding index order on this tensor).\n",
    "\n",
    "- **ContOrder**: a vector containing the positive integer labels from the diagram, used to specify order in which ‘ncon’ contracts the indices. Note that ‘ContOrder’ is an optional input that can be omitted if desired, in which case ‘ncon’ will contract in ascending order of index labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Fig.1.5(b).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.5(b): Contraction using ncon\n",
    "include(\"ncon.jl\")\n",
    "d = 10;\n",
    "A = rand(d,d,d); B = rand(d,d,d,d);\n",
    "C = rand(d,d,d); D = rand(d,d);\n",
    "\n",
    "TensorArray = Any[A,B,C,D];\n",
    "IndexArray = Any[[1,-2,2],[-1,1,3,4],[5,3,2],[4,5]];\n",
    "\n",
    "E = ncon(TensorArray,IndexArray;con_order = [5,3,4,1,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Notes on ncon**\n",
    "\n",
    "- If a pair of tensors is connected via multiple indices then 'ncon' will perform the contraction as a single multiplication (as opposed to contracting each index sequentially).\n",
    "\n",
    "- Can be used to evaluate partial traces (see example below).\n",
    "\n",
    "- Can be used to combine disjoint tensors into a single tensor (see example below). \n",
    "- \n",
    "<img  src=\"./Figs/Fig.1.5(c,d).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.5(c): Partial trace\n",
    "d = 10;\n",
    "A = rand(d,d,d,d,d,d);\n",
    "B = ncon(Any[A],Any[[-1,-2,1,-3,-4,1]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ex.1.5(d): Disjoint networks\n",
    "d = 10;\n",
    "A = rand(d,d);\n",
    "B = rand(d,d);\n",
    "C = ncon(Any[A,B],Any[[-1,-2],[-3,-4]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./Figs/Pb.1(a).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pb.1(b)**\n",
    "\n",
    "Initialize rank-3 random tensors A, B, C (assuming all indices are dimension d = 20). Write code to evaluate the network contraction (using the specified index orders) in three different ways: \n",
    "\n",
    "- As a single summation ​​over all internal indices using FOR loops.\n",
    "\n",
    "- As a sequence of binary contractions implemented using 'permute' and 'reshape'.\n",
    "\n",
    "- Using the 'ncon' routine.\n",
    "\n",
    "Check that all three approaches produce the same output tensor D, and compare their respective computation times. \n",
    "\n",
    "<img  src=\"./Figs/Pb.1(b).png\"  width=\"600\"  align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pb.1(b) - Solutions\n",
    "using LinearAlgebra\n",
    "using ITensors\n",
    "using BenchmarkTools\n",
    "include(\"ncon.jl\")\n",
    "\n",
    "d = 20;\n",
    "A = rand(d,d,d); B = rand(d,d,d); C = rand(d,d,d);\n",
    "i,j,k,l,m,n = Index(d),Index(d),Index(d),Index(d),Index(d),Index(d)\n",
    "IA = ITensor(i,j,k); IB = ITensor(j,l,m); IC = ITensor(k,m,n);\n",
    "for a in 1:d, b in 1:d, c in 1:d\n",
    "    IA[i=>a,j=>b,k=>c] = A[a,b,c]\n",
    "    IB[j=>a,l=>b,m=>c] = B[a,b,c]\n",
    "    IC[k=>a,m=>b,n=>c] = C[a,b,c]\n",
    "end\n",
    "##### Evaluate network via index summation\n",
    "function tempfunct(A,B,C,d)\n",
    "    D0 = zeros(d,d,d);\n",
    "    for b1 = 1:d\n",
    "        for a2 = 1:d\n",
    "            for c3 = 1:d\n",
    "                for a1 = 1:d\n",
    "                    for a3 = 1:d\n",
    "                        for c1 = 1:d\n",
    "                            D0[b1,a2,c3] = D0[b1,a2,c3]+A[a1,a2,a3]*B[b1,a1,c1]*C[c1,a3,c3];\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return D0\n",
    "end\n",
    "\n",
    "t_sum = @elapsed D0 = tempfunct(A,B,C,d);\n",
    "\n",
    "##### Evaluate network using reshape and permute\n",
    "function tempfunct2(A,B,C,d)\n",
    "    Xmid = reshape(reshape(permutedims(B,[1,3,2]),d^2,d)*reshape(A,d,d^2),d,d,d,d);\n",
    "    D1 = reshape(reshape(permutedims(Xmid,[1,3,2,4]),d^2,d^2)*reshape(C,d^2,d),d,d,d);\n",
    "    return D1\n",
    "end\n",
    "\n",
    "##### Evaluate network using ITensors\n",
    "function tempfunct3(A,B,C)\n",
    "    D2 = A*B*C\n",
    "    return D2\n",
    "end\n",
    "\n",
    "t_res = @elapsed D1 = tempfunct2(A,B,C,d)\n",
    "\n",
    "##### Evaluate using ncon\n",
    "t_ncon = @elapsed D2 = ncon(Any[A,B,C],Any[[1,-2,2],[-1,1,3],[3,2,-3]]; con_order = [1,2,3], check_network=true);\n",
    "\n",
    "t_iten = @elapsed D3 = tempfunct3(IA, IB, IC)\n",
    "##### Compare\n",
    "tdiffs = [maximum(abs.(D0[:]-D1[:])),maximum(abs.(D1[:]-D2[:])),maximum(abs.(D2[:]-D0[:]))]\n",
    "ttimes = [t_sum, t_res, t_ncon, t_iten]\n",
    "\n",
    "t_res =  @elapsed D1 = tempfunct2(A,B,C,d)\n",
    "\n",
    "##### Evaluate using ncon\n",
    "t_ncon = @elapsed D2 = ncon(Any[A,B,C],Any[[1,-2,2],[-1,1,3],[3,2,-3]]; con_order = [1,2,3], check_network=true);\n",
    "\n",
    "t_iten = @elapsed D3 = tempfunct3(IA, IB, IC)\n",
    "##### Compare\n",
    "tdiffs = [maximum(abs.(D0[:]-D1[:])),maximum(abs.(D1[:]-D2[:])),maximum(abs.(D2[:]-D0[:]))]\n",
    "ttimes = [t_sum, t_res, t_ncon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark tempfunct(A,B,C,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark tempfunct2(A,B,C,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark ncon(Any[A,B,C],Any[[1,-2,2],[-1,1,3],[3,2,-3]]; con_order = [1,2,3], check_network=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark  tempfunct3(IA, IB, IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp1 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using ITensors\n",
    "using BenchmarkTools\n",
    "include(\"ncon.jl\")\n",
    "\n",
    "d1 = 1000;\n",
    "d2 = 4;\n",
    "A = rand(d1,d2,d1); B = rand(d1,d2,d1);\n",
    "i,j,k,l,m= Index(d1),Index(d2),Index(d1),Index(d2),Index(d1)\n",
    "IA = ITensor(i,j,k); IB = ITensor(k,l,m);\n",
    "for a in 1:d1, b in 1:d2, c in 1:d1\n",
    "    IA[i=>a,j=>b,k=>c] = A[a,b,c]\n",
    "    IB[k=>a,l=>b,m=>c] = B[a,b,c]\n",
    "end\n",
    "\n",
    "\n",
    "function temp1(A,B,d1,d2)\n",
    "    D1 = reshape(reshape(A,d1*d2,d1)*reshape(B,d1,d2*d1),d1,d2,d2,d1);\n",
    "    return D1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 25 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m192.209 ms\u001b[22m\u001b[39m … \u001b[35m207.597 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.15% … 0.89%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m203.918 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.91%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m203.175 ms\u001b[22m\u001b[39m ± \u001b[32m  3.411 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.87% ± 0.19%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[34m \u001b[39m\u001b[39m▃\u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[32m▁\u001b[39m\u001b[39m▇\u001b[34m▇\u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m \u001b[39m▁\n",
       "  192 ms\u001b[90m           Histogram: frequency by time\u001b[39m          208 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m122.07 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m8\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark temp1(A,B,d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 24 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m205.131 ms\u001b[22m\u001b[39m … \u001b[35m260.804 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.34% … 1.69%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m211.121 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.87%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m213.398 ms\u001b[22m\u001b[39m ± \u001b[32m 10.340 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.90% ± 0.33%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[34m█\u001b[39m\u001b[39m▃\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▁\n",
       "  205 ms\u001b[90m           Histogram: frequency by time\u001b[39m          261 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m122.08 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m33\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark IA*IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 21 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m234.535 ms\u001b[22m\u001b[39m … \u001b[35m259.134 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m1.22% … 2.04%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m239.756 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m1.28%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m242.819 ms\u001b[22m\u001b[39m ± \u001b[32m  7.561 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.61% ± 0.59%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  235 ms\u001b[90m           Histogram: frequency by time\u001b[39m          259 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m305.19 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m214\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark ncon(Any[A,B],Any[[-1,-2,1],[1,-3,-4]]; con_order = [1], check_network=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
